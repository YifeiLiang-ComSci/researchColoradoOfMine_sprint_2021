\iffalse
For AD progression prediction using longitudinal phenotypic markers, the input imaging features are a set of matrices X = {X1, X2, . . . , XT } ∈ Rd×n×T corresponding to the measurements at T consecutive time points, where Xt is the phenotypic measurements for a certain type of imaging markers, such as voxel-based morphometry (VBM) markers (see details in Section 3) used in this study, at time t (1 ≤ t ≤ T ).
\fi

\section{Datasets and Problem Formulation}
\subsection{Datasets}
We obtain the data used in this experiment from the ADNI database~\cite{risacher2010longitudinal}. We download 1.5 T MRI scans, single nucleotide polymorphism (SNP), and demographic information (age and gender) of 821 ADNI-1 participants. For the SNP data, the quality control steps discussed in~\cite{shen2010whole} were followed. We perform voxel-based morphometry (VBM) and FreeSurfer (FS) on the MRI data following~\cite{risacher2010longitudinal} and extract mean modulated gray matter measure for 90 target regions of interest. We also downloaded the longitudinal scores of the participants in five independent cognitive assessments including Alzheimer’s Disease Assessment Scale (ADAS), Mini-Mental State Examination (MMSE), Fluency test (FLU), Rey’s Auditory Verbal Learning Test (RAVLT) and Trail making test (TRAILS). 
% We also downloaded the longitudinal scores of the participants in five independent cognitive assessments including Alzheimer’s Disease Assessment Scale (ADAS)~\cite{rosen1984new}, Mini-Mental State Examination (MMSE)~\cite{tombaugh1992mini}, Fluency test (FLU)~\cite{ruff1987ruff}, Rey’s Auditory Verbal Learning Test (RAVLT)~\cite{moradi2017rey} and Trail making test (TRAILS)~\cite{reitan1958validity}. 
% Details about these cognitive assessments are available in the ADNI procedure manuals\footnote{http://adni.loni.usc.edu/wp-content/uploads/2010/09/ADNI\_GeneralProceduresManual.pdf}. 
In this analysis, the time points for both imaging records and cognitive assessments include baseline (BL), month 6 (M6), month 12 (M12), month 18 (M18), and month 24 (M24). We use the diagnosis at month 36 (M36) in Alzheimer's disease (AD), mild cognitive impairment (MCI), and healthy control(HC) as predictive target in our studies. The participants with no missing SNPs, demographic information, and AD diagnosis at M36 were included, providing a set of 379 subjects (104 AD, 115 MCI, 160 HC). Our dataset includes 231 male and 148 female participants, and the average age is 75.35.
% Why multi class?

In the following pages, we denote a vector as a bold lower case letter, and matrix as a bold upper case letter. For the arbitrary matrix $\mathbf{X}$, $[\mathbf{X}]^r$, $[\mathbf{X}]_c$, $[\mathbf{X}]^r_c$ denotes the $r$-th row, $c$-th column, an element of $r$-th row and $c$-th column respectively. We use $i$ and $j$ to index the participants and longitudinal records respectively. We describe the records of $i$-th participant as $\mathcal{X}_i = \{\mathbf{x}_i^b, \mathbf{x}_i^s, \mathbf{X}_i, \mathbf{M}_i, \mathbf{t}_i\}$. $\mathbf{x}_i^b \in \Re^{D_b}$ is a vector of basic demographic information, $\mathbf{x}_i^s \in \Re^{D_s}$ is a vector of SNPs, and $\mathbf{X}_i = [\mathbf{x}_i^1; \mathbf{x}_i^2; \cdots; \mathbf{x}_i^{n_i}] \in \Re^{n_i \times D_l}$ are the longitudinal records collected across the $n_i$ time points, and $\mathbf{M}_i = [\mathbf{m}_i^1; \mathbf{m}_i^2; \cdots; \mathbf{m}_i^{n_i}] \in \{1, 0\}^{n_i \times D_l}$ are the binary masks of longitudinal records $\mathbf{X}_i$, where 1 and 0 indicates the observed and unobserved entry respectively. To fully utilize the multi-modal information, each longitudinal record at $j$-th time point $\mathbf{x}_i^j$ ($1 \leq j \leq n_i$) is the concatenation of multi-modal neuroimagings and cognitive assessments, such that $\mathbf{x}_i^j = [\mathbf{x}_{i,VBM}^j,\mathbf{x}_{i,FS}^j,\mathbf{x}_{i,ADAS}^j,\mathbf{x}_{i,MMSE}^j,\mathbf{x}_{i,FLU}^j,\mathbf{x}_{i,RAVLT}^j,\mathbf{x}_{i,TRAILS}^j] \in \Re^{D_l}$, and the missing records are filled with the constant $-10$. $\mathbf{t}_i = [t_i^1; t_i^2; \cdots; t_i^{n_i}] \in \Re^{n_i}$ are the time stamps of $n_i$ records. The target label $\mathbf{y}_i \in \Re^{D_y}$ of $i$-th participant is given if that participant is in training set, such that $i \in \Omega_{train}$. The target label is one-hot encoded, such that $[1, 0, 0]$, $[0, 1, 0]$, $[0, 0, 1]$ represent AD, MCI, HC respectively. The input and output of the proposed model is described in Fig.~\ref{fig: final-inputs}.
\begin{figure}
    \includegraphics[width=\textwidth]{images/final-inputs.png}
    \caption{Armed with the enriched representations of dynamic and static data, we can fully utilize the information in the dataset. The demographic information is provided without enrichment, as it's dimensionality is relatively small.} \label{fig: final-inputs}
\end{figure}

\subsection{Static Data Enrichment}
We leverage the autoencoder~\cite{kramer1991nonlinear} to learn the abstract representation of genotypic biomarkers. The autoencoder consists of two deep neural networks: an encoder $\phi_{SNP}: \Re^{D_s} \mapsto \Re^{d_s}$ that encodes a vector of SNPs into the enriched abstract representation such that $\phi_{SNP}(\mathbf{x}_i^s;\ \theta^s_{\phi}) = \mathbf{z}_i^s$, and an decoder $\psi_{SNP}: \Re^{d_s} \mapsto \Re^{D_s}$ that decodes the encoded representation into the reconstructed vector of SNPs such that $\psi_{SNP}(\mathbf{z}_i^s;\ \theta^s_{\psi}) = \tilde{\mathbf{x}}_i^s$. $\theta_{\phi}^s$ and $\theta_{\psi}^s$ is the set of trainable variables for encoder and decoder respectively. The deep neural network is defined as the $K$ consecutive layers where the output of $k$-th layer is:
\begin{equation}
    \mathbf{h}_k = \sigma(\mathbf{h}_{k-1}\mathbf{W}_k + \mathbf{b}_k),
\end{equation}
where $\sigma$ is an activation function, and weight matrix $\mathbf{W}_k$ and bias vector $\mathbf{b}_k$ ($1 \leq k \leq K$) are the traiable variables in $\theta_{\phi}^s$ or $\theta_{\psi}^s$.
The input $\mathbf{h}_0$ of the network is then forwarded to the last layer $\mathbf{h}_K = \tilde{\mathbf{x}}_i^s$, which is the output of the network. We train the weights matrices and bias vectors in $\theta_{\phi}^s$ or $\theta_{\psi}^s$ to minimize the following reconstruction loss:
\begin{equation}
    \mathcal{L}_{static}(\mathbf{x}_i^s, \tilde{\mathbf{x}}_i^s;\ \theta_{\phi}^s, \theta_{\psi}^s) = \left\|\mathbf{x}_i^s - \tilde{\mathbf{x}}_i^s\right\|_F^2,
\end{equation}
where squared Frobenious norm $\| \cdot \|_F^2$ is defined as the summation of all the entries squared. By minimizing the reconstruction error, the encoded representation $\mathbf{z}_i^s$ preserves the as much information as possible while removing the redundant noises in SNPs $\mathbf{x}_i^s$.

% then combining it with age and cognitive measures.
% Our models summarizes the dynamic and static data into a fixed-length vectorial representation.
% We use Long Short Term Memory (LSTM) architecture to learn the fixed length vectorial abstract representation summarizing the incomplete time series with varying length and uneven time intervals.
% [In this section, we describe the semi-supervised AutoEncoder (AE) structure to learn a fixed length vectorial representation for each patient's longitudinal records with varying length. The proposed AE is a composite model of three components. The encoder $\phi_E$ learns an abstract enriched representation of the input time series data. The decoder $\phi_D$ learns the reconstruction system to estimate the original data from the enriched representation. The predictor $\phi_P$ learns the prediction system to estimate the target labels from the enriched representation.].

\subsection{Dynamic Data Enrichment}
We choose LSTM encoder $\phi_{dynamic}: \Re^{n_i \times (2 D_l + 1)} \mapsto \Re^{d_l}$ to summarize the longitudinal records and learn the temporal relation between records. The time stamp of each record is crucial in learning the temporal relation between records (e.g. temporal locality), while the the missingness pattern of the entries represents the participants' states. Thus we provide the concatenation of longitudinal records, masks, and time stamps, $[\mathbf{X}_i, \mathbf{M}_i, \mathbf{t}_i] = [\hat{\mathbf{x}}_i^1; \hat{\mathbf{x}}_i^2; \cdots; \hat{\mathbf{x}}_i^{n_i}] = \hat{\mathbf{X}}_i \in \Re^{n_i \times (2 D_l + 1)}$, as an input of LSTM encoder such that $\phi_{dynamic}(\mathbf{X}_i, \mathbf{M}_i, \mathbf{t}_i;\ \theta_{\phi}^l) = \mathbf{z}_i^l$. 

The concatenated longitudinal record at the $j$-th time step $\hat{\mathbf{x}}_i^j$ ($1 \leq j \leq n_i$) is processed by the following LSTM architecture~\cite{yu2019review}:
\begin{equation}
    \mathbf{k}^j_i = \sigma(\hat{\mathbf{x}}^j_i \mathbf{W}_{xk} + \mathbf{h}_i^{j-1} \mathbf{W}_{hk} + \mathbf{c}_i^{j-1} \mathbf{W}_{ck} + \mathbf{b}_k),
\end{equation}
\begin{equation}
    \mathbf{f}^j_i = \sigma(\hat{\mathbf{x}}^j_i \mathbf{W}_{xf} + \mathbf{h}^{j-1}_i \mathbf{W}_{hf} + \mathbf{c}^{j-1}_i \mathbf{W}_{cf} + \mathbf{b}_f),
\end{equation}
\begin{equation}
    \mathbf{c}^j_i = \mathbf{f}^j_i \odot \mathbf{c}^{j-1}_i + \mathbf{k}^j_i \odot \operatorname{tanh}(\hat{\mathbf{x}}_i^j \mathbf{W}_{xc} + \mathbf{h}^{j - 1}_i \mathbf{W}_{hc} + \mathbf{b}_c),
\end{equation}
\begin{equation}
    \mathbf{o}^j_i = \sigma(\hat{\mathbf{x}}_i^j \mathbf{W}_{xo} + \mathbf{h}^{j-1}_i \mathbf{W}_{ho} + \mathbf{c}^j_i \mathbf{W}_{co} + \mathbf{b}_o),
\end{equation}
\begin{equation}
    \mathbf{h}^j_i = \mathbf{o}^j_i \odot \operatorname{tanh}(\mathbf{c}^j_i),
\end{equation}
% \begin{equation}
% \begin{aligned}
%     \mathbf{k}^j_i = \sigma(\hat{\mathbf{x}}^j_i \mathbf{W}_{xk} + \mathbf{h}_i^{j-1} \mathbf{W}_{hk} + \mathbf{c}_i^{j-1} \mathbf{W}_{ck} + \mathbf{b}_k),\ \mathbf{f}^j_i = \sigma(\hat{\mathbf{x}}^j_i \mathbf{W}_{xf} + \mathbf{h}^{j-1}_i \mathbf{W}_{hf} + \mathbf{c}^{j-1}_i \mathbf{W}_{cf} + \mathbf{b}_f),
% \end{aligned}
% \end{equation}
where $\sigma$ and tanh is the logistic sigmoid and hyperbolic tangent activation function respectively, and $\mathbf{k}^j_i$, $\mathbf{o}^j_i$, $\mathbf{f}^j_i$ are input, output, forget gate of $j$-th time step respectively. \{$\mathbf{W}_{xk}$, $\mathbf{W}_{hk}$, $\mathbf{W}_{ck}$, $\mathbf{W}_{xf}$, $\mathbf{W}_{hf}$, $\mathbf{W}_{cf}$, $\mathbf{W}_{xc}$, $\mathbf{W}_{hc}$, $\mathbf{W}_{xo}$, $\mathbf{W}_{ho}$, $\mathbf{W}_{co}$\} $\subset \mathbf{\theta}_{\phi}^l$ are trainable weight matrices and $\{\mathbf{b}_k, \mathbf{b}_f, \mathbf{b}_c, \mathbf{b}_o\} \subset \mathbf{\theta}_{\phi}^l$ are trainable bias vectors. $\mathbf{c}_i^j$ and $\mathbf{h}_i^j$ denote the cell state and hidden representation. The hidden representation $\mathbf{h}_i^{n_i}$ at the last time step $n_i$ represents summarization of the longitudinal records $\hat{\mathbf{X}}_i$, such that $\mathbf{h}_i^{n_i} = \mathbf{z}_i^l \in \Re^{d_l}$. Since the hidden representation at $j$-th time point aims to summarize the records from first time step to $j$-th time step, the LSTM cell needs to refer to the cell state $\mathbf{c}_i^j$ and reflect past records to $\mathbf{h}_i^j$. Since the cell state $\mathbf{c}_i^j$ is guided by the input gate $\mathbf{k}_i^j$ and forget gate $\mathbf{f}_i^j$, which control how much information came from previous step should be preserved, the cell state $\mathbf{c}_i^j$ enables the hidden representation $\mathbf{h}_i^j$ to learn long term dependencies. For example, LSTM encoder can capture the cognitive decline from the temporal trends in the scores of cognitive assessments.

We propose a decoder for dynamic data enrichment with a deep neural network instead of another LSTM. A previous study~\cite{srivastava2015unsupervised} that attempted to enrich longitudinal records with a recurrent neural network (RNN)~\cite{medsker2001recurrent}, did so by using RNNs for both the encoder and decoder, where the output (reconstructed record) of the decoder at each time step depended on the output at the previous time step. However, since no additional information is provided to the decoder other than a time stamp and a learned representation that is no longer longitudinal, there should not be dependency between the outputs of the decoder. Since the enriched representation $\mathbf{z}_i^l$ summarizes \emph{whole} longitudinal records, the decoder for dynamic data enrichment $\psi_{dynamic}:\Re^{d_l + 1} \mapsto \Re^{D_l}$ should be able to reconstruct the $j$-th record $\mathbf{x}_i^j$ given time stamp $t_i^j$ without any additional information, such that $\psi_{dynamic}(\mathbf{z}_i^l, t^j_i;\ \theta^l_{\psi}) = \tilde{\mathbf{x}}_i^j \approx \mathbf{x}_i^j$, where $\theta^l_{\psi}$ is a set of weight matrices and bias vectors of the decoder. This architecture, to the best of our knowledge, has not yet been proposed. We update $\theta^l_{\phi}$ and $\theta^l_{\psi}$ to minimize the error between the reconstructed records and original records for the observed entries indicated by the mask $\mathbf{M}_i$:
\begin{equation}
    \mathcal{L}_{dynamic}(\mathbf{X}_i, \tilde{\mathbf{X}}_i, \mathbf{M}_i;\ 
    \theta^l_{\phi}, \theta^l_{\psi}) = \frac{\left\| (\tilde{\mathbf{X}}_i
- \mathbf{X}_i) \odot \mathbf{M}_i
\right\|_F^2}{\sum_{q=1}^{D_l}\sum_{j=1}^{n_i}[\mathbf{M}_i]^j_q},
\end{equation}
where $\tilde{\mathbf{X}}_i \in \Re^{n_i \times D_l}$ is the stack of reconstructed $n_i$ records for $i$-th participant. 
% Why decoder is not RNN? Think of what is input for encoder and decoder. Because there is no information is additionally provided, current prediction depends on previous prediction does not make sense. The enriched representation is the fixed-length vector which is not longitudinal records anymore.

% Although the enriched representation is learned from a few of observations, it represents the whole temporal dimension of timeseries.
% Given the enriched representation $\mathbf{z}_i^l$, the decoder tries to estimate the function of time $\psi_{dynamic}(t; \mathbf{z}_i^l)$ to reconstruct the state of biomarker representation in temporal dimension.

\begin{figure}
    \includegraphics[width=\textwidth]{images/objective.pdf}
    \caption{An schematic illustration about loss function. Our semi-supervised learning autoencoder minimizes the reconstruction loss for the labeled or unlabeled samples, and prediction loss only for the labeled samples.} \label{fig: objective}
\end{figure}
\subsection{Prediction and Loss Function}
From the enriched representations $\mathbf{z}_i^l$ and $\mathbf{z}_i^s$ of dynamic and static data, another deep neural network $\psi_{pred}: \Re^{(d_s + d_l + D_b)}  \mapsto \Re^{D_y}$ predicts the target label $\mathbf{y}_i$, such that $\psi_{pred}(\mathbf{z}_i^l, \mathbf{z}_i^s, \mathbf{x}_i^b;\ \theta_{\psi}^p) = \tilde{\mathbf{y}}_i$.
We design the loss function to induce the enriched representation to convey the useful information to reconstruct the original records and predict the target label:
\begin{equation}\label{eq: total loss}
\begin{aligned}
    &\theta_{\phi}^s, \theta_{\psi}^s, \theta^l_{\phi}, \theta^l_{\psi}, \theta_{\psi}^p = \argmin_{\theta_{\phi}^s, \theta_{\psi}^s, \theta^l_{\phi}, \theta^l_{\psi}, \theta_{\psi}^p} \bigl(\gamma_1 \mathcal{L}_{predict}(\mathbf{y}_i, \tilde{\mathbf{y}}_i;\ \theta_{\psi}^p) \\
    & + \gamma_2 \mathcal{L}_{static}(\mathbf{x}_i^s, \tilde{\mathbf{x}}_i^s;\ \theta_{\phi}^s, \theta_{\psi}^s) + \gamma_3 \mathcal{L}_{dynamic}(\mathbf{X}_i, \tilde{\mathbf{X}}_i, \mathbf{M}_i;\ \theta^l_{\phi}, \theta^l_{\psi})\bigr),
\end{aligned}
\end{equation}
where $\gamma_1$, $\gamma_2$, and $\gamma_3$ are hyperparameters to adjust the impact of each loss. 
We choose the cross entropy for the prediction loss defined as follows:
\[
\mathcal{L}_{predict}\bigl(\mathbf{y}_i, \tilde{\mathbf{y}}_i;\ \theta_{\psi}^p\bigr)=
\begin{cases}
0 & i \notin \Omega_{train},\\
- \left\|\mathbf{y}_i \odot \operatorname{log}(\tilde{\mathbf{y}}_i) + (\mathbf{1} - \mathbf{y}_i) \odot \operatorname{log}(\mathbf{1} - \tilde{\mathbf{y}}_i)\right\|_1 & i \in \Omega_{train},
\end{cases}
\]
where $\mathbf{1}$ is a vector of 1's and $\operatorname{log}$ is an element-wise logarithm function. 
